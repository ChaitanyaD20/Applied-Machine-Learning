{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfEZJApCrMxZ"
   },
   "source": [
    "# Homework # 4: Unsupervised Learning - Part 2\n",
    "\n",
    "Divide the MNIST dataset into a training and a testing set using 60,000 images for training and\n",
    "the remaining for testing.\n",
    "\n",
    "• Train a Random Forest classifier using the training data and evaluate how long it takes to\n",
    "train.\n",
    "\n",
    "• Test the classifier with the testing data and generate a confusion matrix and compute the\n",
    "overall accuracy.\n",
    "\n",
    "• Use PCA to reduce the dataset’s dimensionality, using explained variance ratio’s of 95%, 90%,\n",
    "and 85%, respectively.\n",
    "\n",
    "• Train Random Forest classifiers using the dimensionally-reduced data and evaluate how long\n",
    "it takes to train. Discuss how the explained variance ratio influences training data, along\n",
    "with a comparison to the initial training time.\n",
    "\n",
    "• Evaluate the classifiers using the testing set, generating the confusion matrices and overall\n",
    "accuracy, for each case. Discuss the performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qE01QBjc8Cl2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlgPXBcnCFFJ",
    "outputId": "65f3d4d0-d811-4966-969c-c6621b3a0b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train),(X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1,X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(-1,X_test.shape[1]*X_test.shape[2])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Impm4bcy0Vv6"
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iGElWFFA0c2t"
   },
   "outputs": [],
   "source": [
    "def calculate_time_for_training(cf, X_train, Y_train):\n",
    "  print()\n",
    "  start_time = time.time()\n",
    "  print ('Training Started')\n",
    "  cf = cf.fit(X_train, Y_train)\n",
    "  end_time = time.time()\n",
    "  print('Time taken for training : ',format(end_time - start_time))\n",
    "  print ('Training completed')\n",
    " \n",
    "  return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "b0bE1g7xD-cs"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(cf, Y_test, y_pred):\n",
    "  print()\n",
    "  scores = accuracy_score(Y_test, y_pred)\n",
    "  print ('Accuracy {0}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "k6D8kzxJ4pg7"
   },
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(Y_test, y_pred):\n",
    "  print()\n",
    "  print(\"Confusion Matrix\")\n",
    "  cm = confusion_matrix(Y_test, y_pred)\n",
    "  print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhHVXMyX3cyL",
    "outputId": "067dbadb-ac82-442a-df0f-ac87642cd2c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Started\n",
      "Time taken for training :  44.99418258666992\n",
      "Training completed\n",
      "\n",
      "Accuracy 0.9689\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 970    0    0    0    0    1    3    1    4    1]\n",
      " [   0 1122    2    3    1    2    3    1    1    0]\n",
      " [   5    0 1002    6    2    0    3    8    6    0]\n",
      " [   0    0   12  972    0    6    0   10    8    2]\n",
      " [   1    0    1    0  954    0    5    0    3   18]\n",
      " [   5    1    0   14    3  854    5    1    6    3]\n",
      " [   7    3    0    0    4    4  937    0    3    0]\n",
      " [   0    3   22    1    0    0    0  987    3   12]\n",
      " [   5    0    5    7    3    7    3    3  932    9]\n",
      " [   5    6    3   11   12    4    1    4    4  959]]\n"
     ]
    }
   ],
   "source": [
    "cf = calculate_time_for_training(classifier, X_train, Y_train) # Calculating time taken\n",
    "\n",
    "y_pred = cf.predict(X_test)\n",
    "calculate_accuracy(cf, Y_test, y_pred) # Calculating Accuracy\n",
    "generate_confusion_matrix(Y_test, y_pred) # Generating confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xlxuRtA3a-g"
   },
   "source": [
    "**Using PCA to reduce dimensionality with variance of 95%, 90% and 85%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WCTxLkZ5Q7I",
    "outputId": "596a1e30-0042-49eb-f545-cc876203297a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with data dimensionality reduced with variance of 95%\n",
      "\n",
      "(60000, 154)\n",
      "(10000, 154)\n",
      "\n",
      "Training Started\n",
      "Time taken for training :  118.5601453781128\n",
      "Training completed\n",
      "\n",
      "Accuracy 0.9486\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 964    0    2    0    1    3    6    1    3    0]\n",
      " [   0 1118    5    3    0    0    4    0    4    1]\n",
      " [   8    0  958   15    8    1    6   11   23    2]\n",
      " [   2    1    8  950    1   15    1    9   19    4]\n",
      " [   1    3    4    0  939    0    9    1    2   23]\n",
      " [   5    1    2   29    4  832    7    2    6    4]\n",
      " [   7    3    2    0    4    7  932    0    2    1]\n",
      " [   1    6   17    3    7    1    0  973    1   19]\n",
      " [   7    0    9   24   11   17    5    8  885    8]\n",
      " [   7    5    2   13   26    3    0   12    6  935]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with data dimensionality reduced with variance of 95%\")\n",
    "print()\n",
    "pca = PCA(.95).fit(X_train)\n",
    "X_train_95 = pca.transform(X_train)\n",
    "X_test_95 = pca.transform(X_test)\n",
    "\n",
    "print(X_train_95.shape)\n",
    "print(X_test_95.shape)\n",
    "\n",
    "cf = calculate_time_for_training(classifier, X_train_95, Y_train)\n",
    "\n",
    "y_pred = cf.predict(X_test_95)\n",
    "calculate_accuracy(cf, Y_test, y_pred)\n",
    "generate_confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zawGPYPH1159",
    "outputId": "b452e7e8-0767-4ff7-ddc2-7d69e649245b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with data dimensionality reduced with variance of 90%\n",
      "\n",
      "(60000, 87)\n",
      "(10000, 87)\n",
      "\n",
      "Training Started\n",
      "Time taken for training :  89.26274871826172\n",
      "Training completed\n",
      "\n",
      "Accuracy 0.9495\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 965    0    2    0    1    3    6    1    2    0]\n",
      " [   0 1122    2    4    0    0    4    0    3    0]\n",
      " [   9    0  968   11    9    1    3    8   23    0]\n",
      " [   1    0    9  957    1   15    3    6   12    6]\n",
      " [   1    2    6    0  930    0    7    4    5   27]\n",
      " [   4    0    2   23    4  833   12    2    5    7]\n",
      " [   7    3    0    0    3    5  939    0    1    0]\n",
      " [   1    8   21    1    6    0    0  965    3   23]\n",
      " [   8    0    9   18   11   23    4    8  885    8]\n",
      " [   4    6    2   13   25    4    1   13   10  931]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with data dimensionality reduced with variance of 90%\")\n",
    "print()\n",
    "pca = PCA(.90).fit(X_train)\n",
    "X_train_90 = pca.transform(X_train)\n",
    "X_test_90 = pca.transform(X_test)\n",
    "\n",
    "print(X_train_90.shape)\n",
    "print(X_test_90.shape)\n",
    "\n",
    "cf = calculate_time_for_training(classifier, X_train_90, Y_train)\n",
    "\n",
    "y_pred = cf.predict(X_test_90)\n",
    "calculate_accuracy(cf, Y_test, y_pred)\n",
    "generate_confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d7vHP35112r",
    "outputId": "32b4029c-4625-46bc-c0be-f478cab6fb4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with data dimensionality reduced with variance of 85%\n",
      "\n",
      "(60000, 59)\n",
      "(10000, 59)\n",
      "\n",
      "Training Started\n",
      "Time taken for training :  68.8673906326294\n",
      "Training completed\n",
      "\n",
      "Accuracy 0.9536\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 965    0    2    0    0    5    5    2    1    0]\n",
      " [   0 1119    4    4    0    1    3    0    3    1]\n",
      " [   9    0  973   13    5    2    2    8   19    1]\n",
      " [   1    0   11  946    1   15    3    7   21    5]\n",
      " [   1    1    4    0  942    2    7    1    3   21]\n",
      " [   3    1    4   24    6  838    6    1    6    3]\n",
      " [   6    2    1    0    3    3  941    0    1    1]\n",
      " [   1    4   18    1    4    0    1  975    2   22]\n",
      " [   8    0    8   15    8   17    4    7  900    7]\n",
      " [   6    6    2   11   23    8    0    9    7  937]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with data dimensionality reduced with variance of 85%\")\n",
    "print()\n",
    "pca = PCA(.85).fit(X_train)\n",
    "X_train_85 = pca.transform(X_train)\n",
    "X_test_85 = pca.transform(X_test)\n",
    "\n",
    "print(X_train_85.shape)\n",
    "print(X_test_85.shape)\n",
    "\n",
    "cf = calculate_time_for_training(classifier, X_train_85, Y_train)\n",
    "\n",
    "y_pred = cf.predict(X_test_85)\n",
    "calculate_accuracy(cf, Y_test, y_pred)\n",
    "generate_confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ptgKRE3gvQO"
   },
   "source": [
    "                          variance    time     accuracy   feature\n",
    "                          normal      44.99     96.9%      784\n",
    "                          95          118.56    94.8%      154\n",
    "                          90          89.26     94.9%      87\n",
    "                          85          68.86     95.3%      59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c34pHPv1iKmS"
   },
   "source": [
    "- As we can see from the above resuts, the original dataset with 784 features take around 46 seconds for training with accuracy of 94.5%. This is the most accurate with least time as compared to all other different variance ratios.\n",
    "\n",
    "- As the variance ratio reduces, the features or dimensions also reduces. This change reflects in the time and accuracy of training our model for different variance ratios.\n",
    "\n",
    "- Now, since the features/dimensions are reduced from 784 to 154, 87 and 59, we would expect the time taken for training would decrease and the accuracy of our model would increase. But we see the contradiction over here. \n",
    "\n",
    "- As the dimesions are reduced, the time taken for training increases from the original dataset and also the accuracy drops from that of the original one.\n",
    "\n",
    "- This happens only when we are training with 0.95 variance that increases the training time and further reducing it to 0.90 and 0.85 decreases the training time and increases the accuracy as expected. Although, this is not better than that of the original dataset but close enough.\n",
    "\n",
    "- The reason for this unexpected behaviour can be due to PCA dimension reduction, model reduces the information from the original dataset and only uses portion of data for training. \n",
    "\n",
    "- This may cause some of the features to be missing or losing out some of the data points which might have helped in finding good splits and training the model and eventually increasing the training time of our model.\n",
    "\n",
    "- This is becuause the classifier tries to find the good split by considering all of the features in the reduced dataset or losing some data points. Since some of the features are reduced, we may have missed out on a feature that might have resulted in finding a good split for our training model. For this reasons, the overall time of the training increasea and the accuracy goes down.\n",
    "\n",
    "- Overall, PCA reduced dimensionality model performed worse than the original model using Random forest classifier for the above said reasons. Although the decision tree was expected to be build quicker for reduced feature, it tries to find the most accurate feature to perform the split and since we might have lost that during reduction, we have to perform more processing to find the optimum split. The reason for model performing well for 0.90 and 0.85 after it being worse for 0.95 from the original dataset is due to the fact that we are already in the similar dimensions and hence this will help in quickly finding the good feature to perform the split.\n",
    "\n",
    "- Hence, PCA with correct dimensions or variance might help in training our model better with reduced time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "joWMSiK_AszY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw4-Part2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
